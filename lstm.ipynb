{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing\n",
    "df = pd.read_csv('./data/model_data.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Define input and output features (excluding 'date' for model training)\n",
    "input_features = [\n",
    "    'precipitation + irrigation (mm)',\n",
    "    'potential evapotranspiration (mm)',\n",
    "    # 'year',\n",
    "    # 'month',\n",
    "    # 'day',\n",
    "    # 'day_of_week',\n",
    "    # 'day_of_year',\n",
    "    'precip_pet_diff',\n",
    "    'precip_7d_avg',\n",
    "    'pet_7d_avg',\n",
    "    # 'precip_7d_sum',\n",
    "    # 'pet_7d_sum'\n",
    "]\n",
    "\n",
    "output_features = [\n",
    "    'depth 10cm',\n",
    "    'depth 30cm',\n",
    "    'depth 60cm',\n",
    "    'depth 90cm',\n",
    "    'actual evapotranspiration (mm)',\n",
    "    'groundwater recharge (mm)'\n",
    "]\n",
    "\n",
    "# Convert DataFrame columns to NumPy arrays\n",
    "X = df[input_features].values\n",
    "y = df[output_features].values\n",
    "\n",
    "# Scale the features for better LSTM performance\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "# Create Sequences for LSTM Input\n",
    "def create_sequences(X, y, lookback=7):\n",
    "    \"\"\"\n",
    "    Create sequences of length 'lookback' from the time series data.\n",
    "    For each sample, the input is the previous 'lookback' days\n",
    "    and the output is the target for the day following the sequence.\n",
    "    \"\"\"\n",
    "    Xs, ys = [], []\n",
    "    for i in range(lookback, len(X)):\n",
    "        Xs.append(X[i-lookback:i])\n",
    "        ys.append(y[i])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "lookback = 7  # Use the past 7 days to predict the next day\n",
    "X_seq, y_seq = create_sequences(X_scaled, y_scaled, lookback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test Split (Time-based)\n",
    "split_idx = int(0.8 * len(X_seq))\n",
    "X_train, X_test = X_seq[:split_idx], X_seq[split_idx:]\n",
    "y_train, y_test = y_seq[:split_idx], y_seq[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the LSTM Model\n",
    "model = Sequential()\n",
    "# First LSTM layer; returns sequences for the next LSTM layer\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, \n",
    "               input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "# Second LSTM layer; does not return sequences\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "# Dense layer for multi-output regression (6 target variables)\n",
    "model.add(Dense(len(output_features)))\n",
    "\n",
    "# Set a custom learning rate\n",
    "learning_rate = 0.00005  # Try values like 0.0001, 0.0005, 0.005, 0.01, etc.\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# Compile model with custom learning rate\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Model\n",
    "history = model.fit(X_train, y_train, epochs=30, batch_size=32, \n",
    "                    validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and Evaluate\n",
    "# Predict on the test set\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "# Inverse transform to get predictions in the original scale\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_inv = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate using Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test_inv, y_pred)\n",
    "print(\"Test MSE:\", mse) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. predicted values for each target variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(y_test_inv[:, i], label='Actual', color='blue', linewidth=2)\n",
    "    ax.plot(y_pred[:, i], label='Predicted', color='red', linewidth=2)\n",
    "    ax.set_title(output_features[i])\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE for each target variable\n",
    "mse_per_target = {}\n",
    "for i, feature in enumerate(output_features):\n",
    "    mse_per_target[feature] = mean_squared_error(y_test_inv[:, i], y_pred[:, i])\n",
    "    # print(f\"{feature}: {mse_per_target[feature]:.2f}\")\n",
    "\n",
    "\n",
    "print(\"Mean Squared Error for each target variable:\")\n",
    "for feature, mse_val in mse_per_target.items():\n",
    "    print(f\"   {feature}: {mse_val:.2f}\")\n",
    "print(\"\\nOverall Mean Squared Error:\", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
